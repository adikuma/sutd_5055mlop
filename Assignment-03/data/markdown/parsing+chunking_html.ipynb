{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "sutd_data = load_data('markdown_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(data):\n",
    "    processed_docs = []\n",
    "\n",
    "    for item in data:\n",
    "        # Skip entries with empty markdown content\n",
    "        if not item.get('markdown'):\n",
    "            continue\n",
    "\n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            'title': item.get('title', ''),\n",
    "            'url': item.get('url', ''),\n",
    "            'description': item.get('description', ''),\n",
    "            'pillar': extract_pillar(item.get('title', ''), item.get('url', ''))\n",
    "        }\n",
    "\n",
    "        # Add the markdown content with normalized headers\n",
    "        content = normalize_headers(item.get('markdown', ''))\n",
    "\n",
    "        # Create a LangChain Document object\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "    return processed_docs\n",
    "\n",
    "def extract_pillar(title, url):\n",
    "    \"\"\"Extract the pillar/department from title or URL\"\"\"\n",
    "    pillars = ['ISTD', 'ESD', 'EPD', 'ASD', 'DAI', 'HASS', 'SMT']\n",
    "\n",
    "    for pillar in pillars:\n",
    "        if pillar in title or pillar.lower() in url.lower():\n",
    "            return pillar\n",
    "    return 'General'\n",
    "\n",
    "def normalize_headers(markdown_text):\n",
    "    \"\"\"Ensure headers are properly formatted for splitting\"\"\"\n",
    "    # Make sure there's a space after # characters for proper header parsing\n",
    "    markdown_text = re.sub(r'(#{1,6})([^#\\s])', r'\\1 \\2', markdown_text)\n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = process_documents(sutd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function process_documents at 0x109681e40>\n"
     ]
    }
   ],
   "source": [
    "print(process_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents saved to parsed_docs.json\n"
     ]
    }
   ],
   "source": [
    "def document_to_dict(doc):\n",
    "    return {\n",
    "        'page_content': doc.page_content,\n",
    "        'metadata': dict(doc.metadata)\n",
    "    }\n",
    "\n",
    "output_path = \"parsed_docs.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    # Convert each Document to a dictionary before saving\n",
    "    serializable_docs = [document_to_dict(doc) for doc in processed_docs]\n",
    "    json.dump(serializable_docs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Processed documents saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 27\n",
      "Keys in first document: dict_keys(['page_content', 'metadata'])\n",
      "Metadata fields: dict_keys(['title', 'url', 'description', 'pillar'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON data\n",
    "with open('parsed_docs.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Preview the data structure\n",
    "print(f\"Total documents: {len(data)}\")\n",
    "print(f\"Keys in first document: {data[0].keys()}\")\n",
    "print(f\"Metadata fields: {data[0]['metadata'].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_markdown_hierarchy(markdown_text):\n",
    "    \"\"\"\n",
    "    Parse markdown text to extract hierarchical structure based on headings.\n",
    "    Returns a list of sections with their headings, content, and parent headings.\n",
    "    \"\"\"\n",
    "    # Convert markdown to HTML for easier parsing\n",
    "    html = markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    sections = []\n",
    "    current_section = {\"title\": \"Root\", \"level\": 0, \"content\": \"\", \"parents\": []}\n",
    "    heading_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "\n",
    "    # First pass: identify all headings and their levels\n",
    "    headings = []\n",
    "    for tag in soup.find_all(heading_tags):\n",
    "        level = int(tag.name[1])\n",
    "        headings.append({\n",
    "            \"tag\": tag,\n",
    "            \"title\": tag.get_text().strip(),\n",
    "            \"level\": level,\n",
    "        })\n",
    "\n",
    "    # If no headings found, treat the entire document as one section\n",
    "    if not headings:\n",
    "        return [{\"title\": \"Root\", \"level\": 0, \"content\": markdown_text, \"parents\": []}]\n",
    "\n",
    "    # Second pass: extract section content and build hierarchy\n",
    "    for i, heading in enumerate(headings):\n",
    "        # Find content up to the next heading or end of document\n",
    "        content_elements = []\n",
    "        element = heading[\"tag\"].next_sibling\n",
    "\n",
    "        while element and (i == len(headings) - 1 or element != headings[i+1][\"tag\"]):\n",
    "            if element.name not in heading_tags:\n",
    "                if hasattr(element, 'get_text'):\n",
    "                    content_elements.append(str(element))\n",
    "            element = element.next_sibling\n",
    "\n",
    "        # Get parent headings\n",
    "        parent_titles = []\n",
    "        for prev_heading in reversed(headings[:i]):\n",
    "            if prev_heading[\"level\"] < heading[\"level\"]:\n",
    "                parent_titles.insert(0, prev_heading[\"title\"])\n",
    "\n",
    "        # Build section\n",
    "        section = {\n",
    "            \"title\": heading[\"title\"],\n",
    "            \"level\": heading[\"level\"],\n",
    "            \"content\": ''.join(content_elements),\n",
    "            \"parents\": parent_titles\n",
    "        }\n",
    "        sections.append(section)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_internal_urls(content):\n",
    "    \"\"\"\n",
    "    Extract all internal URLs (links to other SUTD pages) from the content.\n",
    "    \"\"\"\n",
    "    # Pattern to match markdown links\n",
    "    pattern = r'\\[.*?\\]\\((https?://www\\.sutd\\.edu\\.sg/[^)]+)\\)'\n",
    "\n",
    "    # Find all matches\n",
    "    urls = re.findall(pattern, content)\n",
    "\n",
    "    # Also check for HTML links if any HTML is embedded in the markdown\n",
    "    if '<a href=\"' in content:\n",
    "        html_pattern = r'<a href=\"(https?://www\\.sutd\\.edu\\.sg/[^\"]+)\"'\n",
    "        html_urls = re.findall(html_pattern, content)\n",
    "        urls.extend(html_urls)\n",
    "\n",
    "    return list(set(urls))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 213\n"
     ]
    }
   ],
   "source": [
    "def process_document(doc):\n",
    "    \"\"\"\n",
    "    Process a single document into chunks with enhanced metadata.\n",
    "    \"\"\"\n",
    "    # Extract basic metadata\n",
    "    metadata = doc['metadata']\n",
    "    content = doc['page_content']\n",
    "\n",
    "    # Extract markdown structure\n",
    "    sections = extract_markdown_hierarchy(content)\n",
    "\n",
    "    # Process each section into a chunk\n",
    "    chunks = []\n",
    "    for section in sections:\n",
    "        # Skip very short sections (likely just headings)\n",
    "        if len(section[\"content\"]) < 10 and section[\"level\"] > 0:\n",
    "            continue\n",
    "\n",
    "        # Reconstruct full text for this section\n",
    "        section_title = f\"# {section['title']}\" if section[\"level\"] > 0 else \"\"\n",
    "        section_text = f\"{section_title}\\n\\n{section['content']}\"\n",
    "\n",
    "        # Extract internal URLs\n",
    "        internal_urls = extract_internal_urls(section_text)\n",
    "\n",
    "        # Create enhanced metadata\n",
    "        enhanced_metadata = {\n",
    "            **metadata,  # Original metadata (title, url, description, pillar)\n",
    "            'section_title': section['title'],\n",
    "            'parent_sections': section['parents'],\n",
    "            'section_level': section['level'],\n",
    "            'internal_links': internal_urls\n",
    "        }\n",
    "\n",
    "        # Add to chunks\n",
    "        chunks.append({\n",
    "            'text': section_text.strip(),\n",
    "            'metadata': enhanced_metadata\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Process all documents\n",
    "all_chunks = []\n",
    "for doc in data:\n",
    "    doc_chunks = process_document(doc)\n",
    "    all_chunks.extend(doc_chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total refined chunks: 270\n"
     ]
    }
   ],
   "source": [
    "def refine_chunks(chunks, min_size=100, max_size=1000):\n",
    "    \"\"\"\n",
    "    Refine chunks to ensure they're within optimal size limits.\n",
    "    Combines small chunks and splits large ones.\n",
    "    \"\"\"\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    refined_chunks = []\n",
    "    current_text = \"\"\n",
    "    current_metadata = None\n",
    "\n",
    "    # First pass: combine small chunks with the same parent\n",
    "    for chunk in chunks:\n",
    "        # If chunk is too small and has the same parent as previous chunk\n",
    "        if (len(chunk['text']) < min_size and current_metadata and\n",
    "            chunk['metadata']['parent_sections'] == current_metadata['parent_sections']):\n",
    "            # Append to current text\n",
    "            current_text += \"\\n\\n\" + chunk['text']\n",
    "        else:\n",
    "            # Add previous combined chunk if it exists\n",
    "            if current_text:\n",
    "                refined_chunks.append({\n",
    "                    'text': current_text,\n",
    "                    'metadata': current_metadata\n",
    "                })\n",
    "\n",
    "            # Start new current chunk\n",
    "            current_text = chunk['text']\n",
    "            current_metadata = chunk['metadata']\n",
    "\n",
    "    # Add the last combined chunk if it exists\n",
    "    if current_text:\n",
    "        refined_chunks.append({\n",
    "            'text': current_text,\n",
    "            'metadata': current_metadata\n",
    "        })\n",
    "\n",
    "    # Second pass: split large chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_size,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    final_chunks = []\n",
    "    for chunk in refined_chunks:\n",
    "        if len(chunk['text']) > max_size:\n",
    "            # Split the text\n",
    "            split_texts = text_splitter.split_text(chunk['text'])\n",
    "\n",
    "            # Create new chunks with the same metadata\n",
    "            for split_text in split_texts:\n",
    "                final_chunks.append({\n",
    "                    'text': split_text,\n",
    "                    'metadata': chunk['metadata']\n",
    "                })\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "# Refine the chunks\n",
    "refined_chunks = refine_chunks(all_chunks)\n",
    "print(f\"Total refined chunks: {len(refined_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cleaned chunks: 270\n"
     ]
    }
   ],
   "source": [
    "def clean_chunk_text(chunks):\n",
    "    \"\"\"\n",
    "    Clean HTML and markdown formatting from chunk text while preserving content.\n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "\n",
    "    cleaned_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Get the original text\n",
    "        original_text = chunk['text']\n",
    "\n",
    "        # Step 1: Clean markdown headings\n",
    "        # Remove heading markers like \"# About SUTD\"\n",
    "        text = re.sub(r'^\\s*#+\\s+(.+?)$', r'\\1', original_text, flags=re.MULTILINE)\n",
    "\n",
    "        # Step 2: Parse and clean HTML\n",
    "        # First ensure we're parsing as HTML\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "        # Get text content - this removes all HTML tags\n",
    "        clean_text = soup.get_text()\n",
    "\n",
    "        # Step 3: Fix spacing and formatting\n",
    "        # Replace multiple newlines with at most two\n",
    "        clean_text = re.sub(r'\\n{3,}', '\\n\\n', clean_text)\n",
    "\n",
    "        # Remove leading/trailing whitespace\n",
    "        clean_text = clean_text.strip()\n",
    "\n",
    "        # Create new chunk with cleaned text\n",
    "        cleaned_chunk = {\n",
    "            'text': clean_text,\n",
    "            'metadata': chunk['metadata']  # Keep original metadata\n",
    "        }\n",
    "\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "\n",
    "    return cleaned_chunks\n",
    "\n",
    "# Add this after refining the chunks\n",
    "cleaned_chunks = clean_chunk_text(refined_chunks)\n",
    "print(f\"Total cleaned chunks: {len(cleaned_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 23 chunks are very short (<50 chars)\n",
      "Chunk statistics:\n",
      "{\n",
      "  \"count\": 270,\n",
      "  \"min_length\": 6,\n",
      "  \"max_length\": 995,\n",
      "  \"avg_length\": 397.4,\n",
      "  \"median_length\": 317,\n",
      "  \"pillars\": {\n",
      "    \"General\": 97,\n",
      "    \"ISTD\": 44,\n",
      "    \"ASD\": 23,\n",
      "    \"DAI\": 30,\n",
      "    \"EPD\": 37,\n",
      "    \"ESD\": 39\n",
      "  }\n",
      "}\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Length: 330\n",
      "Title: Specialisation Tracks\n",
      "Pillar: EPD\n",
      "Internal links: []\n",
      "First 100 chars: # Specialisation Tracks\n",
      "\n",
      "\n",
      "<ul>\n",
      "<li>Beyond Industry 4.0</li>\n",
      "<li>Computer Engineering</li>\n",
      "<li>Electr...\n",
      "\n",
      "Chunk 2:\n",
      "Length: 965\n",
      "Title: Curriculum Structure\n",
      "Pillar: ESD\n",
      "Internal links: ['https://www.sutd.edu.sg/course/40-319-statistical-and-machine-learning/', 'https://www.sutd.edu.sg/course/40-316-game-theory/', 'https://www.sutd.edu.sg/course/50-045-information-retrieval/', 'https://www.sutd.edu.sg/course/50-039-theory-and-practice-of-deep-learning/', 'https://www.sutd.edu.sg/course/30-100-computational-and-data-driven-engineering/', 'https://www.sutd.edu.sg/course/50-055-special-topic-machine-learning-operations/', 'https://www.sutd.edu.sg/course/50-043-database-systems/', 'https://www.sutd.edu.sg/course/50-040-natural-language-processing/', 'https://www.sutd.edu.sg/course/50-050-discrete-mathematics-and-algorithm-design/', 'https://www.sutd.edu.sg/course/10-020-data-driven-world-elective/', 'https://www.sutd.edu.sg/course/10-022-modelling-uncertainty/', 'https://www.sutd.edu.sg/course/50-035-computer-vision/', 'https://www.sutd.edu.sg/course/30-114-advanced-feedback-and-control/', 'https://www.sutd.edu.sg/course/30-115-digital-signal-processing/', 'https://www.sutd.edu.sg/course/50-021-artificial-intelligence/', 'https://www.sutd.edu.sg/course/40-016-the-analytics-edge/', 'https://www.sutd.edu.sg/course/50-007-machine-learning/', 'https://www.sutd.edu.sg/course/40-002-optimisation/', 'https://www.sutd.edu.sg/course/50-038-computational-data-science/']\n",
      "First 100 chars: <li><strong>ESD/ISTD/EPD Cross Pillar Electives:</strong> Choose up to twenty-four (24) credits from...\n",
      "\n",
      "Chunk 3:\n",
      "Length: 166\n",
      "Title: Specialisation Tracks\n",
      "Pillar: ISTD\n",
      "Internal links: ['https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/financial-technology/', 'https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/security/', 'https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/visual-analytics-and-computing/', 'https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/data-analytics', 'https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/iot-and-intelligent-systems/', 'https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/software-engineering/', 'https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/custom-track/', 'https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/artificial-intelligence/']\n",
      "First 100 chars: <li><a href=\"https://www.sutd.edu.sg/istd/education/undergraduate/specialisation-tracks/visual-analy...\n",
      "\n",
      "Chunk 4:\n",
      "Length: 118\n",
      "Title: Student Awards\n",
      "Pillar: EPD\n",
      "Internal links: []\n",
      "First 100 chars: # Student Awards\n",
      "\n",
      "\n",
      "<ul>\n",
      "<li><strong>Student Academic Awards</strong></li>\n",
      "<li><strong>Honours List</...\n",
      "\n",
      "Chunk 5:\n",
      "Length: 237\n",
      "Title: Academic Calendar\n",
      "Pillar: ISTD\n",
      "Internal links: ['https://www.sutd.edu.sg/istd/education/undergraduate/academic-calendar/overview/ay2024-onwards/']\n",
      "First 100 chars: # Academic Calendar\n",
      "\n",
      "\n",
      "<p>For more details about your school terms, vacation periods, and more, visit...\n"
     ]
    }
   ],
   "source": [
    "def analyze_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Analyze chunks to ensure quality and identify any issues.\n",
    "    \"\"\"\n",
    "    # Get length statistics\n",
    "    lengths = [len(chunk['text']) for chunk in chunks]\n",
    "\n",
    "    stats = {\n",
    "        'count': len(chunks),\n",
    "        'min_length': min(lengths),\n",
    "        'max_length': max(lengths),\n",
    "        'avg_length': sum(lengths) / len(chunks),\n",
    "        'median_length': sorted(lengths)[len(lengths) // 2],\n",
    "        'pillars': {}\n",
    "    }\n",
    "\n",
    "    # Count chunks by pillar\n",
    "    for chunk in chunks:\n",
    "        pillar = chunk['metadata']['pillar']\n",
    "        if pillar not in stats['pillars']:\n",
    "            stats['pillars'][pillar] = 0\n",
    "        stats['pillars'][pillar] += 1\n",
    "\n",
    "    # Check for very short chunks\n",
    "    short_chunks = [chunk for chunk in chunks if len(chunk['text']) < 50]\n",
    "    if short_chunks:\n",
    "        print(f\"Warning: {len(short_chunks)} chunks are very short (<50 chars)\")\n",
    "\n",
    "    # Verification: Check if important URLs are preserved\n",
    "    urls_present = any('internal_links' in chunk['metadata'] and chunk['metadata']['internal_links']\n",
    "                     for chunk in chunks)\n",
    "    if not urls_present:\n",
    "        print(\"Warning: No internal links found in chunk metadata\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Analyze the chunks\n",
    "chunk_stats = analyze_chunks(refined_chunks)\n",
    "print(\"Chunk statistics:\")\n",
    "print(json.dumps(chunk_stats, indent=2))\n",
    "\n",
    "# Save a few example chunks for inspection\n",
    "import random\n",
    "sample_chunks = random.sample(refined_chunks, min(5, len(refined_chunks)))\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(sample_chunks):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Length: {len(chunk['text'])}\")\n",
    "    print(f\"Title: {chunk['metadata']['section_title']}\")\n",
    "    print(f\"Pillar: {chunk['metadata']['pillar']}\")\n",
    "    print(f\"Internal links: {chunk['metadata'].get('internal_links', [])}\")\n",
    "    print(f\"First 100 chars: {chunk['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 270 chunks to processed_sutd_chunks.json\n"
     ]
    }
   ],
   "source": [
    "def export_chunks(chunks, output_file=\"processed_sutd_chunks.json\"):\n",
    "    \"\"\"\n",
    "    Export processed chunks to a JSON file.\n",
    "    \"\"\"\n",
    "    # Convert chunks to a serializable format\n",
    "    serializable_chunks = []\n",
    "    for chunk in chunks:\n",
    "        serializable_chunks.append({\n",
    "            'text': chunk['text'],\n",
    "            'metadata': chunk['metadata']\n",
    "        })\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Exported {len(chunks)} chunks to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "# Export the refined chunks\n",
    "output_file = export_chunks(refined_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
