{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d2bc81-e67b-413d-b520-38dfe92b2e23",
   "metadata": {},
   "source": [
    "# Group Project / Assignment 3: Retrieval-Augmented Generation Question Answering\n",
    "**Assignment due 6 April 11:59pm 2025**\n",
    "\n",
    "Welcome to the third assignment for 50.055 Machine Learning Operations. \n",
    "The third and fourth assignment together form the course group project. You will be working in your project groups to build a chatbot which can answer questions about SUTD to prospective students.\n",
    "\n",
    "\n",
    "**This assignment is a group assignment.**\n",
    "\n",
    "- Read the instructions in this notebook carefully\n",
    "- Add your solution code and answers in the appropriate places. The questions are marked as **QUESTION:**, the places where you need to add your code and text answers are marked as **ADD YOUR SOLUTION HERE**\n",
    "- The completed notebook, including your added code and generated output will be your submission for the assignment.\n",
    "- The notebook should execute without errors from start to finish when you select \"Restart Kernel and Run All Cells..\". Please test this before submission.\n",
    "- Use the SUTD Education Cluster to solve and test the assignment. If you work on another environment, minimally test your work on the SUTD Education Cluster.\n",
    "\n",
    "**Rubric for assessment** \n",
    "\n",
    "Your submission will be graded using the following criteria. \n",
    "1. Code executes: your code should execute without errors. The SUTD Education cluster should be used to ensure the same execution environment.\n",
    "2. Correctness: the code should produce the correct result or the text answer should state the factual correct answer.\n",
    "3. Style: your code should be written in a way that is clean and efficient. Your text answers should be relevant, concise and easy to understand.\n",
    "4. Partial marks will be awarded for partially correct solutions.\n",
    "5. Creativity and innovation: in this assignment you have more freedom to design your solution, compared to the first assignments. You can show of your creativity and innovative mindset. \n",
    "6. There is a maximum of 225 points for this assignment.\n",
    "\n",
    "**ChatGPT policy** \n",
    "\n",
    "If you use AI tools, such as ChatGPT, to solve the assignment questions, you need to be transparent about its use and mark AI-generated content as such. In particular, you should include the following in addition to your final answer:\n",
    "- A copy or screenshot of the prompt you used\n",
    "- The name of the AI model\n",
    "- The AI generated output\n",
    "- An explanation why the answer is correct or what you had to change to arrive at the correct answer\n",
    "\n",
    "**Assignment Notes:** Please make sure to save the notebook as you go along. Submission instructions are located at the bottom of the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf8b8a-1385-47bf-9aea-68bac46f7098",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG) \n",
    "\n",
    "In this assignment, you will be building a Retrieval-Augmented Generation (RAG) question answering system which can answer questions about SUTD.\n",
    "\n",
    "We'll be leveraging `langchain` and `llama 3.2`.\n",
    "\n",
    "Check out the docs:\n",
    "- [LangChain](https://docs.langchain.com/docs/)\n",
    "- [Llama 3.2](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/)\n",
    "\n",
    "\n",
    "The SUTD website used to allow chatting with current students. Unfortunately, this feature does not exist anymore. Let's build a chatbot to fill this gap!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada99010",
   "metadata": {},
   "source": [
    "### Conduct user research\n",
    "\n",
    "What are the questions that prospective and current students have about SUTD? In week 2, you already conducted some user research to understand your users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd4e96",
   "metadata": {},
   "source": [
    "### Value Proposition Canvas\n",
    "\n",
    "\n",
    "### QUESTION: \n",
    "\n",
    "Paste the value proposition canvas which you have created in week 2 into this notebook below. \n",
    "\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (10 points) ---**\n",
    "\n",
    "- (replace canvas image below)\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5ec91",
   "metadata": {},
   "source": [
    "![image.png](images/canvas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1d0c3",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "Use pip to install all required dependencies of this assignment in the cell below. Make sure to test this on the SUTD cluster as different environments have different software pre-installed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1ed5c8e-f07e-4de2-b2de-0a063001668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==1.6.0 (from -r requirements.txt (line 1))\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting aiofiles==24.1.0 (from -r requirements.txt (line 2))\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohappyeyeballs==2.6.1 (from -r requirements.txt (line 3))\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiohttp==3.11.14 (from -r requirements.txt (line 4))\n",
      "  Using cached aiohttp-3.11.14-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting aiosignal==1.3.2 (from -r requirements.txt (line 5))\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (0.7.0)\n",
      "Collecting antlr4-python3-runtime==4.9.3 (from -r requirements.txt (line 7))\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio==4.9.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (4.9.0)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (3.0.0)\n",
      "Collecting attrs==25.3.0 (from -r requirements.txt (line 10))\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting backoff==2.2.1 (from -r requirements.txt (line 11))\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting beautifulsoup4==4.13.3 (from -r requirements.txt (line 12))\n",
      "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement bitsandbytes==0.45.4 (from versions: 0.31.8, 0.32.0, 0.32.1, 0.32.2, 0.32.3, 0.33.0, 0.33.1, 0.34.0, 0.35.0, 0.35.1, 0.35.2, 0.35.3, 0.35.4, 0.36.0, 0.36.0.post1, 0.36.0.post2, 0.37.0, 0.37.1, 0.37.2, 0.38.0, 0.38.0.post1, 0.38.0.post2, 0.38.1, 0.39.0, 0.39.1, 0.40.0, 0.40.0.post1, 0.40.0.post2, 0.40.0.post3, 0.40.0.post4, 0.40.1, 0.40.1.post1, 0.40.2, 0.41.0, 0.41.1, 0.41.2, 0.41.2.post1, 0.41.2.post2, 0.41.3, 0.41.3.post1, 0.41.3.post2, 0.42.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for bitsandbytes==0.45.4\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# QUESTION: Install and import all required packages\n",
    "# The rest of your code should execute without any import or dependency errors.\n",
    "\n",
    "# **--- ADD YOUR SOLUTION HERE (10 points) ---**\n",
    "!pip3 install -r requirements.txt\n",
    "\n",
    "# For CUDA purposes, uncomment the following line and run it in your terminal:\n",
    "#! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb2b9c",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f8b8f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (2.2.4)\n",
      "Requirement already satisfied: torch in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (4.50.3)\n",
      "Requirement already satisfied: markdown in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (3.7)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (4.13.3)\n",
      "Requirement already satisfied: sentence-transformers in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (4.0.2)\n",
      "Requirement already satisfied: faiss-cpu in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (1.10.0)\n",
      "Requirement already satisfied: openai in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (1.70.0)\n",
      "Requirement already satisfied: langchain in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (0.3.23)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (0.3.51)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: python-dotenv in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: scikit-learn in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from openai) (2.11.2)\n",
      "Requirement already satisfied: sniffio in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from langchain) (0.3.24)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from langchain) (2.0.40)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Using cached aiohttp-3.11.16-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from langchain-core) (1.33)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Using cached tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached multidict-6.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
      "Using cached aiohttp-3.11.16-cp313-cp313-macosx_11_0_arm64.whl (453 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Using cached tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl (1.0 MB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl (50 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.3.2-cp313-cp313-macosx_11_0_arm64.whl (35 kB)\n",
      "Using cached propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl (91 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: propcache, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, tiktoken, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-openai, langchain-community\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.5.0 httpx-sse-0.4.0 langchain-community-0.3.21 langchain-openai-0.3.12 marshmallow-3.26.1 multidict-6.3.2 mypy-extensions-1.0.0 propcache-0.3.1 pydantic-settings-2.8.1 tiktoken-0.9.0 typing-inspect-0.9.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install \\\n",
    "pandas \\\n",
    "numpy \\\n",
    "torch \\\n",
    "transformers \\\n",
    "markdown \\\n",
    "beautifulsoup4 \\\n",
    "sentence-transformers \\\n",
    "faiss-cpu \\\n",
    "openai \\\n",
    "langchain \\\n",
    "langchain-community \\\n",
    "langchain-core \\\n",
    "langchain-openai \\\n",
    "python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59b15dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from glob import glob\n",
    "import openai\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from markdown import markdown\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "from rag_prompt import RAG_PROMPT\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "MARKDOWN_PATH = \"data/markdown/markdown_data.json\"\n",
    "HTML_PATH = \"data/html/html_data.json\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "TOP_K = 5\n",
    "OUTPUT_DIR = \"vector_store\"\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "MODEL_DIR = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3447d-b41f-4ac5-95b9-1cae3a42d620",
   "metadata": {},
   "source": [
    "# Download documents\n",
    "The RAG application should be able to answer questions based on ingested documents. For the SUTD chatbot, download PDF and HTML files from the SUTD website. The documents should contain information about the admission process, available courses and the university in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7c5201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 30 HTML documents and 30 Markdown documents\n",
      "Found 27 Markdown files on disk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Has Markdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUTD About page</td>\n",
       "      <td>https://www.sutd.edu.sg/about/</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUTD Contact page</td>\n",
       "      <td>https://www.sutd.edu.sg/contact-us/contact-sutd/</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUTD Home page</td>\n",
       "      <td>https://www.sutd.edu.sg/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUTD Application Guide page</td>\n",
       "      <td>https://www.sutd.edu.sg/admissions/undergradua...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUTD Appeal Guide page</td>\n",
       "      <td>https://www.sutd.edu.sg/admissions/undergradua...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SUTD Admission Requirements page</td>\n",
       "      <td>https://www.sutd.edu.sg/admissions/undergradua...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SUTD Masters information page 1</td>\n",
       "      <td>https://www.sutd.edu.sg/admissions/graduate/ma...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SUTD Masters information page 2</td>\n",
       "      <td>https://www.sutd.edu.sg/admissions/graduate/ma...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SUTD PHD information page</td>\n",
       "      <td>https://www.sutd.edu.sg/admissions/graduate/phd/</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SUTD Academic Calendar</td>\n",
       "      <td>https://www.sutd.edu.sg/education/undergraduat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Title  \\\n",
       "0                   SUTD About page   \n",
       "1                 SUTD Contact page   \n",
       "2                    SUTD Home page   \n",
       "3       SUTD Application Guide page   \n",
       "4            SUTD Appeal Guide page   \n",
       "5  SUTD Admission Requirements page   \n",
       "6   SUTD Masters information page 1   \n",
       "7   SUTD Masters information page 2   \n",
       "8         SUTD PHD information page   \n",
       "9            SUTD Academic Calendar   \n",
       "\n",
       "                                                 URL  Has Markdown  \n",
       "0                     https://www.sutd.edu.sg/about/          True  \n",
       "1   https://www.sutd.edu.sg/contact-us/contact-sutd/          True  \n",
       "2                           https://www.sutd.edu.sg/         False  \n",
       "3  https://www.sutd.edu.sg/admissions/undergradua...          True  \n",
       "4  https://www.sutd.edu.sg/admissions/undergradua...          True  \n",
       "5  https://www.sutd.edu.sg/admissions/undergradua...          True  \n",
       "6  https://www.sutd.edu.sg/admissions/graduate/ma...         False  \n",
       "7  https://www.sutd.edu.sg/admissions/graduate/ma...          True  \n",
       "8   https://www.sutd.edu.sg/admissions/graduate/phd/          True  \n",
       "9  https://www.sutd.edu.sg/education/undergraduat...          True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and 20 more documents\n",
      "\n",
      "Document Statistics:\n",
      "Total SUTD documents: 30\n",
      "Documents with extracted markdown content: 27\n"
     ]
    }
   ],
   "source": [
    "# QUESTION: Download documents from the SUTD website\n",
    "# You should download at least 10 documents but more documents can increase the knowledge base of your chatbot.\n",
    "\n",
    "# **--- ADD YOUR SOLUTION HERE (20 points) ---**\n",
    "def get_data(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        output_json = json.load(f)\n",
    "    return output_json\n",
    "\n",
    "html_data = get_data(HTML_PATH)\n",
    "markdown_data = get_data(MARKDOWN_PATH)\n",
    "\n",
    "print(\n",
    "    f\"Successfully loaded {len(html_data)} HTML documents and {len(markdown_data)} Markdown documents\"\n",
    ")\n",
    "\n",
    "markdown_files = glob(\"data/markdown/*.md\")\n",
    "print(f\"Found {len(markdown_files)} Markdown files on disk\")\n",
    "\n",
    "docs_info = []\n",
    "for doc in markdown_data:\n",
    "    docs_info.append(\n",
    "        {\n",
    "            \"Title\": doc.get(\"title\", \"No Title\"),\n",
    "            \"URL\": doc.get(\"url\", \"No URL\"),\n",
    "            \"Has Markdown\": bool(doc.get(\"markdown\", \"\").strip()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "docs_df = pd.DataFrame(docs_info)\n",
    "display(docs_df.head(10))\n",
    "\n",
    "if len(docs_df) > 10:\n",
    "    print(f\"... and {len(docs_df) - 10} more documents\")\n",
    "\n",
    "print(\"\\nDocument Statistics:\")\n",
    "print(f\"Total SUTD documents: {len(docs_df)}\")\n",
    "print(f\"Documents with extracted markdown content: {docs_df['Has Markdown'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a646cbd-2a05-4078-995b-4317a94ed109",
   "metadata": {},
   "source": [
    "# Split documents\n",
    "Use LangChain to split the documents into smaller text chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40aa4eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Using cached unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Using cached lxml-5.3.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.7 kB)\n",
      "Collecting nltk (from unstructured)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured) (4.13.3)\n",
      "Collecting emoji (from unstructured)\n",
      "  Using cached emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Using cached python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured) (2.2.4)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Using cached rapidfuzz-3.13.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured) (4.13.1)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Using cached unstructured_client-0.32.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Using cached wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tqdm in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured) (7.0.0)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Using cached python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured)\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Collecting webencodings (from html5lib->unstructured)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting click (from nltk->unstructured)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Using cached olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from requests->unstructured) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from requests->unstructured) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from requests->unstructured) (2025.1.31)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cryptography>=3.1 (from unstructured-client->unstructured)\n",
      "  Using cached cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from unstructured-client->unstructured)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (2.11.2)\n",
      "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
      "  Using cached pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (0.4.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Using cached cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: anyio in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from pydantic>=2.10.3->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from pydantic>=2.10.3->unstructured-client->unstructured) (2.33.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/weimingchin/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Using cached unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Using cached lxml-5.3.1-cp313-cp313-macosx_10_13_universal2.whl (8.2 MB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Using cached python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Using cached rapidfuzz-3.13.0-cp313-cp313-macosx_11_0_arm64.whl (1.4 MB)\n",
      "Using cached unstructured_client-0.32.1-py3-none-any.whl (180 kB)\n",
      "Using cached wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl (6.7 MB)\n",
      "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: webencodings, filetype, wrapt, rapidfuzz, python-magic, python-iso639, pypdf, pycparser, olefile, lxml, langdetect, html5lib, eval-type-backport, emoji, click, chardet, backoff, aiofiles, python-oxmsg, nltk, cffi, cryptography, unstructured-client, unstructured\n",
      "Successfully installed aiofiles-24.1.0 backoff-2.2.1 cffi-1.17.1 chardet-5.2.0 click-8.1.8 cryptography-44.0.2 emoji-2.14.1 eval-type-backport-0.2.2 filetype-1.2.0 html5lib-1.1 langdetect-1.0.9 lxml-5.3.1 nltk-3.9.1 olefile-0.47 pycparser-2.22 pypdf-5.4.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 unstructured-0.17.2 unstructured-client-0.32.1 webencodings-0.5.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27ed0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "####LATEST CHUNKING CODE#####\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# QUESTION: Use langchain to split the documents into chunks\n",
    "\n",
    "# --- ADD YOUR SOLUTION HERE (20 points)---\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "def extract_pillar(title, url):\n",
    "    # extract each pillar to add to metadata to make it easier to provide context to LLM\n",
    "    pillars = [\"ISTD\", \"ESD\", \"EPD\", \"ASD\", \"DAI\", \"HASS\", \"SMT\"]\n",
    "\n",
    "    for pillar in pillars:\n",
    "        if pillar in title or pillar.lower() in url.lower():\n",
    "            return pillar\n",
    "    # if no pillar is found, return 'General'\n",
    "    return \"General\"\n",
    "\n",
    "#extract all internal urls in a page\n",
    "def extract_all_internal_urls(markdown_text):\n",
    "    internal_urls = []\n",
    "\n",
    "    link_pattern = r'\\[([^\\]]+)\\]\\((https?://[^)]+)\\)'\n",
    "    matches = re.findall(link_pattern, markdown_text)\n",
    "\n",
    "    for text, url in matches:\n",
    "        if 'sutd.edu.sg' in url:\n",
    "            internal_urls.append({\n",
    "                'text': text,\n",
    "                'url': url\n",
    "            })\n",
    "\n",
    "    return internal_urls\n",
    "\n",
    "#match urls to chunk title\n",
    "def match_urls_to_chunk(chunk_title, all_urls):\n",
    "    if not chunk_title:\n",
    "        return []\n",
    "\n",
    "    matched_urls = []\n",
    "\n",
    "    normalized_title = chunk_title.lower()\n",
    "\n",
    "    for url_info in all_urls:\n",
    "        link_text = url_info['text'].lower()\n",
    "\n",
    "        if link_text == normalized_title:\n",
    "            matched_urls.append(url_info['url'])\n",
    "            continue\n",
    "\n",
    "        title_words = set(normalized_title.split())\n",
    "        link_words = set(link_text.split())\n",
    "\n",
    "        common_words = title_words.intersection(link_words)\n",
    "        if len(common_words) >= min(2, len(title_words) // 2):\n",
    "            matched_urls.append(url_info['url'])\n",
    "\n",
    "    return matched_urls\n",
    "\n",
    "for item in markdown_data:\n",
    "    # if markdown is empty, continue\n",
    "    if not item.get(\"markdown\"):\n",
    "        continue\n",
    "\n",
    "    all_internal_urls = extract_all_internal_urls(item[\"markdown\"])\n",
    "\n",
    "    # create temporary file for each markdown page as the input for UnstructuredMarkdownLoader\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as temp_file:\n",
    "        temp_file.write(item[\"markdown\"])\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    try:\n",
    "        # call UnstructuredMarkdownLoader class to parse markdown files and split file by \"elements\" structure\n",
    "        loader = UnstructuredMarkdownLoader(temp_file_path, mode=\"elements\")\n",
    "        docs = loader.load()\n",
    "\n",
    "        chunk_groups = {}\n",
    "        current_title = None\n",
    "\n",
    "        # access to individual \"chunks\" of data after split by elements\n",
    "        for doc in docs:\n",
    "\n",
    "            if doc.metadata.get(\"category\") == \"Title\":\n",
    "                current_title = doc.page_content\n",
    "                # doc.metadata[\"_skip\"] = True\n",
    "                continue\n",
    "\n",
    "            if current_title not in chunk_groups:\n",
    "                chunk_groups[current_title] = []\n",
    "\n",
    "            chunk_groups[current_title].append(doc.page_content)\n",
    "        for title, contents in chunk_groups.items():\n",
    "\n",
    "            combined_content = \" \".join(contents)\n",
    "\n",
    "            if len(combined_content.split()) < 3:  # skip if fewer than 3 words\n",
    "                continue\n",
    "\n",
    "            relevant_urls = match_urls_to_chunk(current_title, all_internal_urls)\n",
    "\n",
    "            enhanced_metadata = {\n",
    "                \"title\": item[\"title\"],\n",
    "                \"url\": item[\"url\"],\n",
    "                \"description\": item[\"description\"],\n",
    "\n",
    "                \"element_type\": doc.metadata.get(\"category\", \"unknown\"),\n",
    "\n",
    "                \"chunk_title\": title,\n",
    "\n",
    "                #TODO: Match all internal URLS according to words similarity to current_title\n",
    "\n",
    "                \"internal_urls\": relevant_urls if relevant_urls else None,\n",
    "\n",
    "                \"pillar\": extract_pillar(item.get(\"title\", \"\"), item.get(\"url\", \"\"))\n",
    "\n",
    "            }\n",
    "\n",
    "            enhanced_doc = Document(\n",
    "                page_content=combined_content,\n",
    "                metadata=enhanced_metadata\n",
    "            )\n",
    "\n",
    "            all_documents.append(enhanced_doc)\n",
    "\n",
    "    finally:\n",
    "        # clean up the temporary files\n",
    "        os.unlink(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5175131-4a38-4861-96f4-456efea1a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents: 27\n",
      "Total chunks created: 213\n",
      "page_content='# About SUTD\n",
      "\n",
      "\n",
      "<p>SUTD integrates design, AI and technology into a holistic, interdisciplinary education and research experience. This unique approach encourages our students to push the boundaries of innovating solutions to real-world problems.</p>' metadata={'title': 'SUTD About page', 'url': 'https://www.sutd.edu.sg/about/', 'description': 'Provides an overview of SUTD, its mission, and its unique educational approach.', 'pillar': 'General', 'section_title': 'About SUTD', 'parent_sections': [], 'section_level': 1, 'internal_links': []}\n",
      "Total refined chunks: 270\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# # QUESTION: Use langchain to split the documents into chunks\n",
    "\n",
    "# # --- ADD YOUR SOLUTION HERE (20 points)---\n",
    "# def process_documents(data):\n",
    "#     processed_docs = []\n",
    "\n",
    "#     # take in json data (dictionary) and create a list of documents\n",
    "#     for item in data:\n",
    "#         # get the markdown content\n",
    "#         if not item.get(\"markdown\"):\n",
    "#             continue\n",
    "\n",
    "#         # extract the metadata from the json\n",
    "#         metadata = {\n",
    "#             \"title\": item.get(\"title\", \"\"),\n",
    "#             \"url\": item.get(\"url\", \"\"),\n",
    "#             \"description\": item.get(\"description\", \"\"),\n",
    "#             \"pillar\": extract_pillar(item.get(\"title\", \"\"), item.get(\"url\", \"\")),\n",
    "#         }\n",
    "\n",
    "#         # normalize headers\n",
    "#         content = normalize_headers(item.get(\"markdown\", \"\"))\n",
    "\n",
    "#         # create langchain document object\n",
    "#         doc = Document(page_content=content, metadata=metadata)\n",
    "\n",
    "#         processed_docs.append(doc)\n",
    "#     return processed_docs\n",
    "\n",
    "\n",
    "# def extract_pillar(title, url):\n",
    "#     # extract each pillar to add to metadata to make it easier to provide context to LLM\n",
    "#     pillars = [\"ISTD\", \"ESD\", \"EPD\", \"ASD\", \"DAI\", \"HASS\", \"SMT\"]\n",
    "\n",
    "#     for pillar in pillars:\n",
    "#         if pillar in title or pillar.lower() in url.lower():\n",
    "#             return pillar\n",
    "#     # if no pillar is found, return 'General'\n",
    "#     return \"General\"\n",
    "\n",
    "\n",
    "# def normalize_headers(markdown_text):\n",
    "#     # add a space after # characters for proper header parsing\n",
    "#     markdown_text = re.sub(r\"(#{1,6})([^#\\s])\", r\"\\1 \\2\", markdown_text)\n",
    "#     return markdown_text\n",
    "\n",
    "\n",
    "# def extract_markdown_hierarchy(markdown_text):\n",
    "#     # convert the markdown text to html\n",
    "#     html = markdown(markdown_text)\n",
    "#     soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#     sections = []\n",
    "#     current_section = {\"title\": \"Root\", \"level\": 0, \"content\": \"\", \"parents\": []}\n",
    "#     # list of header tags to look for\n",
    "#     heading_tags = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "\n",
    "#     # first pass: identify all headings and their levels\n",
    "#     headings = []\n",
    "#     for tag in soup.find_all(heading_tags):\n",
    "#         level = int(tag.name[1])\n",
    "#         headings.append(\n",
    "#             {\n",
    "#                 \"tag\": tag,\n",
    "#                 \"title\": tag.get_text().strip(),\n",
    "#                 \"level\": level,\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     # if there were no headings found, treat the entire document as one section\n",
    "#     if not headings:\n",
    "#         current_section[\"content\"] = markdown_text\n",
    "#         return [current_section]\n",
    "\n",
    "#     # second pass: extract section content and build hierarchy\n",
    "#     for i, heading in enumerate(headings):\n",
    "#         # find content up to the next heading or end of document\n",
    "#         content_elements = []\n",
    "#         element = heading[\"tag\"].next_sibling\n",
    "\n",
    "#         while element and (i == len(headings) - 1 or element != headings[i + 1][\"tag\"]):\n",
    "#             if element.name not in heading_tags:\n",
    "#                 if hasattr(element, \"get_text\"):\n",
    "#                     content_elements.append(str(element))\n",
    "#             element = element.next_sibling\n",
    "\n",
    "#         # get the parent headings\n",
    "#         parent_titles = []\n",
    "#         for prev_heading in reversed(headings[:i]):\n",
    "#             if prev_heading[\"level\"] < heading[\"level\"]:\n",
    "#                 parent_titles.insert(0, prev_heading[\"title\"])\n",
    "\n",
    "#         # build section\n",
    "#         section = {\n",
    "#             \"title\": heading[\"title\"],\n",
    "#             \"level\": heading[\"level\"],\n",
    "#             \"content\": \"\".join(content_elements),\n",
    "#             \"parents\": parent_titles,\n",
    "#         }\n",
    "#         sections.append(section)\n",
    "#     return sections\n",
    "\n",
    "# # the markdowns kept in some internal URLs which are useful\n",
    "# def extract_internal_urls(content):\n",
    "#     # since the markdown has a few links to internal pages, we need to extract them\n",
    "#     # pattern to match markdown links\n",
    "#     pattern = r\"\\[.*?\\]\\((https?://www\\.sutd\\.edu\\.sg/[^)]+)\\)\"\n",
    "#     urls = re.findall(pattern, content)\n",
    "\n",
    "#     # also check for HTML links if any HTML is embedded in the markdown\n",
    "#     if '<a href=\"' in content:\n",
    "#         html_pattern = r'<a href=\"(https?://www\\.sutd\\.edu\\.sg/[^\"]+)\"'\n",
    "#         html_urls = re.findall(html_pattern, content)\n",
    "#         urls.extend(html_urls)\n",
    "\n",
    "#     return list(set(urls))\n",
    "\n",
    "\n",
    "# def process_document(doc):\n",
    "#     # extract the metadata from the document\n",
    "#     metadata = doc.metadata\n",
    "#     content = doc.page_content\n",
    "\n",
    "#     # extract markdown structure\n",
    "#     sections = extract_markdown_hierarchy(content)\n",
    "\n",
    "#     # process each section into a chunk\n",
    "#     chunks = []\n",
    "#     for section in sections:\n",
    "#         # skip very short sections\n",
    "#         if len(section[\"content\"]) < 10 and section[\"level\"] > 0:\n",
    "#             continue\n",
    "\n",
    "#         # get the full text for this section\n",
    "#         section_title = f\"# {section['title']}\" if section[\"level\"] > 0 else \"\"\n",
    "#         section_text = f\"{section_title}\\n\\n{section['content']}\"\n",
    "\n",
    "#         # extract all urls internal to the section\n",
    "#         internal_urls = extract_internal_urls(section_text)\n",
    "\n",
    "#         # create the metadata\n",
    "#         enhanced_metadata = {\n",
    "#             **metadata,\n",
    "#             \"section_title\": section[\"title\"],\n",
    "#             \"parent_sections\": section[\"parents\"],\n",
    "#             \"section_level\": section[\"level\"],\n",
    "#             \"internal_links\": internal_urls,\n",
    "#         }\n",
    "\n",
    "#         chunks.append({\"text\": section_text.strip(), \"metadata\": enhanced_metadata})\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# def refine_chunks(chunks, min_size=100, max_size=1000):\n",
    "#     refined_chunks = []\n",
    "#     current_text = \"\"\n",
    "#     current_metadata = None\n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         if (\n",
    "#             len(chunk[\"text\"]) < min_size\n",
    "#             and current_metadata\n",
    "#             and chunk[\"metadata\"][\"parent_sections\"]\n",
    "#             == current_metadata[\"parent_sections\"]\n",
    "#         ):\n",
    "#             current_text += \"\\n\\n\" + chunk[\"text\"]\n",
    "#         else:\n",
    "#             if current_text:\n",
    "#                 refined_chunks.append(\n",
    "#                     Document(page_content=current_text, metadata=current_metadata)\n",
    "#                 )\n",
    "\n",
    "#             current_text = chunk[\"text\"]\n",
    "#             current_metadata = chunk[\"metadata\"]\n",
    "\n",
    "#     if current_text:\n",
    "#         refined_chunks.append(Document(page_content=current_text, metadata=current_metadata))\n",
    "\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=max_size, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "#     )\n",
    "#     print(refined_chunks[0])\n",
    "\n",
    "#     final_chunks = text_splitter.split_documents(refined_chunks)\n",
    "#     return final_chunks\n",
    "\n",
    "\n",
    "# processed_docs = process_documents(markdown_data)\n",
    "# print(f\"Processed documents: {len(processed_docs)}\")\n",
    "\n",
    "# all_chunks = []\n",
    "# for doc in processed_docs:\n",
    "#     doc_chunks = process_document(doc)\n",
    "#     all_chunks.extend(doc_chunks)\n",
    "\n",
    "# print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# refined_chunks = refine_chunks(all_chunks)\n",
    "# print(f\"Total refined chunks: {len(refined_chunks)}\")\n",
    "# print(type(refined_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85abee2",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "\n",
    "What chunking method or strategy did you use? Why did you use this method. Explain your design decision in less than 10 sentences.\n",
    "\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (10 points) ---**\n",
    "\n",
    "We designed a hierarchical chunking strategy that respects the natural flow of a document. First, we extract logical sections using markdown headers (h1-h6), turning each section into its own chunk with its heading, content, and useful metadata like title, URL, parent sections, and pillar/department information. To avoid fragmentation, we combine very short chunks (under 100 characters) with nearby related content, and for long chunks (over 1000 characters), we split them using a recursive approach that breaks at natural separators such as paragraphs or sentences. We also keep a 100-character overlap between chunks to maintain context. This way, each chunk remains a complete, meaningful unit, perfectly sized for embedding and retrieval.\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a133cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is the implementation for using LangChain to create a vector store\n",
    "# But it didn't allow us to switch between different embeddings easily with various models\n",
    "\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embedding = OpenAIEmbeddings(model=EMBEDDING_MODEL, openai_api_key=OPENAI_API_KEY)\n",
    "# retriever = FAISS.from_documents(refined_chunks, embedding).as_retriever(search_kwargs={\"k\": 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "891962a0-65ac-48c5-8a36-6424f0e14734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example chunk: page_content='SUTD integrates design, AI and technology into a holistic, interdisciplinary education and research experience. This unique approach encourages our students to push the boundaries of innovating solutions to real-world problems.' metadata={'title': 'SUTD About page', 'url': 'https://www.sutd.edu.sg/about/', 'description': 'Provides an overview of SUTD, its mission, and its unique educational approach.', 'element_type': 'UncategorizedText', 'chunk_title': 'About SUTD', 'internal_urls': None, 'pillar': 'General'}\n",
      "\n",
      "embedding 202 items with model: sentence-transformers/all-MiniLM-L6-v2\n",
      "saved vector store with 202 documents\n"
     ]
    }
   ],
   "source": [
    "# QUESTION: create embeddings of document chunks and store them in a local vector store for fast lookup\n",
    "# Decide an appropriate embedding model. Use Huggingface to run the embedding model locally.\n",
    "# You do not have to use cloud-based APIs.\n",
    "\n",
    "# --- ADD YOUR SOLUTION HERE (20 points)---\n",
    "# QUESTION: create embeddings of document chunks and store them in a local vector store for fast lookup\n",
    "# Decide an appropriate embedding model. Use Huggingface to run the embedding model locally.\n",
    "# You do not have to use cloud-based APIs.\n",
    "# --- ADD YOUR SOLUTION HERE (20 points)---\n",
    "\n",
    "\n",
    "\n",
    "# creating a retriever class to make it compatible with langchain and allow for huggingface embeddings and openai embeddings\n",
    "# this class that can handle both local Hugging Face models AND OpenAI models in one implementation (we made this to make it easy for us to switch between models)\n",
    "# the best part is storing the embeddings in a local vector store (i think langchain has this as well but we made our own)\n",
    "# we just added this layer on top of the simple langchain implementation\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        if model_name.startswith(\"text-embedding-\"):\n",
    "            self.model = None\n",
    "        else:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        print(f\"\\nembedding {len(texts)} items with model: {self.model_name}\")\n",
    "        if self.model_name.startswith(\"text-embedding-\"):\n",
    "            response = openai.embeddings.create(model=self.model_name, input=texts)\n",
    "            return [r.embedding for r in response.data]\n",
    "        else:\n",
    "            embeddings = self.model.encode(\n",
    "                texts, convert_to_numpy=True, normalize_embeddings=True\n",
    "            ).astype(\"float32\")\n",
    "            return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# using FAISS to create a vector store from the documents and using it's as_retriever method to create a retriever\n",
    "# setting default k to 7\n",
    "# embedding model chosen is text-embedding-ada-002\n",
    "def create_retriever(refined_chunks, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\", k=7):\n",
    "    embeddings = CustomEmbeddings(model_name=embedding_model)\n",
    "\n",
    "    if len(refined_chunks) > 0:\n",
    "        print(f\"example chunk: {refined_chunks[0]}\")\n",
    "\n",
    "    vectorstore = FAISS.from_documents(refined_chunks, embeddings)\n",
    "    vector_store_dir = OUTPUT_DIR\n",
    "    os.makedirs(vector_store_dir, exist_ok=True)\n",
    "    vectorstore.save_local(vector_store_dir)\n",
    "\n",
    "    print(f\"saved vector store with {len(refined_chunks)} documents\")\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "retriever = create_retriever(all_documents, embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6bb601a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "`FlashrankRerank` is not fully defined; you should define `Ranker`, then call `FlashrankRerank.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/class-not-fully-defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPydanticUserError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpretty_print_docs\u001b[39m(docs):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m      9\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     10\u001b[39m             [\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         )\n\u001b[32m     15\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m compressor = \u001b[43mFlashrankRerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m compression_retriever = ContextualCompressionRetriever(\n\u001b[32m     19\u001b[39m     base_compressor=compressor, base_retriever=retriever\n\u001b[32m     20\u001b[39m )\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/sutd_5055mlop/.venv/lib/python3.13/site-packages/pydantic/_internal/_mock_val_ser.py:100\u001b[39m, in \u001b[36mMockValSer.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# raise an AttributeError if `item` doesn't exist\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._val_or_ser, item)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\u001b[38;5;28mself\u001b[39m._error_message, code=\u001b[38;5;28mself\u001b[39m._code)\n",
      "\u001b[31mPydanticUserError\u001b[39m: `FlashrankRerank` is not fully defined; you should define `Ranker`, then call `FlashrankRerank.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/class-not-fully-defined"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "\n",
    "# the idea here is to rerank those candidates with a more powerful model (slower but more accurate)\n",
    "# we think that the retriever may not be able to pick the best documents hence use a reranker to pick the best documents\n",
    "# this is done by using the FlashrankRerank class\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "compressor = FlashrankRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f20674",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "\n",
    "What embeddings and vector store did you use and why? Explain your design decision in less than 10 sentences.\n",
    "\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (10 points) ---**\n",
    "\n",
    "We used OpenAI's \"text-embedding-ada-002\" for embeddings and FAISS (Facebook AI Similarity Search) as our vector store. The \"text-embedding-ada-002\" model provides embeddings with high quality semantic understanding of academic and technical content, which is important for accurately representing SUTD's educational information. We have compared the performance of this model against the following other models namely: all-MiniLM-L6-v2, all-mpnet-base-v2, and text-embedding-3-small. Among the 4, text-embedding-ada-002 was able to perform the best (based on human judgement) and it also integrates smoothly alongside the vector store chosen.\n",
    "\n",
    "FAISS was our go-to choice because it efficiently handles nearest-neighbor searches in high-dimensional spaces, retrieves results quickly even from large collections, and keeps memory usage low through quantization. We have done research on other strategies such as Pinecone and Weaviate, but the ease of integration with FAISS utlimately helped us make our decision. Moreover, we have come across a lot of research in which FAISS was used, which proves its reliability.\n",
    "\n",
    "We built a custom embeddings class that works with both OpenAI and local HuggingFace models, so switching between them is seamless while using the same interface. This setup delivers fast, accurate semantic search results while reliably keeping the vector store locally.\n",
    "\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff0a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: When was SUTD founded?\n",
      "\n",
      "embedding 1 items with model: text-embedding-ada-002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "# About SUTD\n",
      "\n",
      "\n",
      "<p>SUTD integrates design, AI and technology into a holistic, interdisciplinary education and research experience. This unique approach encourages our students to push the boundaries of innovating solutions to real-world problems.</p>\n",
      "Metadata: {'id': 0, 'relevance_score': np.float32(0.99885666), 'title': 'SUTD About page', 'url': 'https://www.sutd.edu.sg/about/', 'description': 'Provides an overview of SUTD, its mission, and its unique educational approach.', 'pillar': 'General', 'section_title': 'About SUTD', 'parent_sections': [], 'section_level': 1, 'internal_links': []}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "# Quick Links\n",
      "\n",
      "\n",
      "<ul>\n",
      "<li><a href=\"https://www.sutd.edu.sg/about/partnering-with-sutd/giving/\">Donate to SUTD</a></li>\n",
      "<li><a href=\"https://www.sutd.edu.sg/enterprise/research-collaborations/\">Research collaboration</a></li>\n",
      "<li><a href=\"https://www.sutd.edu.sg/enterprise/technology-licensing/\">Technology licensing</a></li>\n",
      "<li><a href=\"https://www.sutd.edu.sg/enterprise/design-innovation/\">Design innovation consultancy</a></li>\n",
      "<li><a href=\"https://www.sutd.edu.sg/education/undergraduate/capstone/\">Student Capstone projects</a></li>\n",
      "</ul>\n",
      "Metadata: {'id': 5, 'relevance_score': np.float32(0.991826), 'title': 'SUTD About page', 'url': 'https://www.sutd.edu.sg/about/', 'description': 'Provides an overview of SUTD, its mission, and its unique educational approach.', 'pillar': 'General', 'section_title': 'Quick Links', 'parent_sections': ['About SUTD', 'Our Vision', 'Our Mission', 'Core Values', 'Latest Happenings'], 'section_level': 3, 'internal_links': ['https://www.sutd.edu.sg/enterprise/design-innovation/', 'https://www.sutd.edu.sg/education/undergraduate/capstone/', 'https://www.sutd.edu.sg/enterprise/research-collaborations/', 'https://www.sutd.edu.sg/about/partnering-with-sutd/giving/', 'https://www.sutd.edu.sg/enterprise/technology-licensing/']}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "# Contact SUTD\n",
      "\n",
      "\n",
      "<p>Thank you for your interest in SUTD. We are located at:</p>\n",
      "<p><strong>Address:</strong> 8 Somapah Road Singapore 487372<br/>\n",
      "<strong>Tel:</strong> +65 6303 6600 / +65 6303 6655<br/>\n",
      "<strong>Email:</strong> <a href=\"mailto:enquiry@sutd.edu.sg\">enquiry@sutd.edu.sg</a> / <a href=\"mailto:admissions@sutd.edu.sg\">admissions@sutd.edu.sg</a></p>\n",
      "Metadata: {'id': 1, 'relevance_score': np.float32(0.9899743), 'title': 'SUTD Contact page', 'url': 'https://www.sutd.edu.sg/contact-us/contact-sutd/', 'description': 'Offers various methods to contact SUTD, including phone numbers, email addresses, and physical addresses.', 'pillar': 'General', 'section_title': 'Contact SUTD', 'parent_sections': [], 'section_level': 1, 'internal_links': []}\n"
     ]
    }
   ],
   "source": [
    "# Execute a query against the vector store\n",
    "\n",
    "query = \"When was SUTD founded?\"\n",
    "\n",
    "# QUESTION: run the query against the vector store, print the top 5 search results\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points)---\n",
    "# TODO: manually add in when SUTD was founded to the dataset\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# use the query_index function to get the top k results\n",
    "response = compression_retriever.invoke(query)\n",
    "pretty_print_docs(response)\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ce000",
   "metadata": {},
   "source": [
    "## Huggingface Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0492e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "003e2cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8d966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3e5be828ef463fadc53dc81ce70870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:\n",
      "What courses are available in SUTD??\n",
      "Singapore University of Technology and Design (SUTD) offers a wide range of undergraduate and graduate courses across various disciplines. Here are some of the courses available in SUTD:\n",
      "\n",
      "**Undergraduate Courses**\n",
      "\n",
      "1. Bachelor of Science in Information Technology (BSIT)\n",
      "2. Bachelor of Science in Information Systems (BSIS)\n",
      "3. Bachelor of Science in Information Technology and Systems (BSITS)\n",
      "4. Bachelor of Science in Data Science (BSDS)\n",
      "5. Bachelor of Science in Artificial Intelligence and Data Science (BSAIDS)\n",
      "6. Bachelor of Science in Human-Computer Interaction (BSHCI)\n",
      "7. Bachelor of Science in Information Technology and Systems (BSITS)\n",
      "8. Bachelor of Science in Computer Science (BSCS)\n",
      "9. Bachelor of Science in Engineering (BSE)\n",
      "10. Bachelor of Science in Design (BSD)\n",
      "11. Bachelor of Science in Architecture (BSA)\n",
      "12. Bachelor of Science in Biomedical Engineering (BSBME)\n",
      "13. Bachelor of Science in Electrical Engineering (BSEE)\n",
      "14. Bachelor of Science in Mechanical Engineering (BSME)\n",
      "15. Bachelor of Science in Civil Engineering (BSCE)\n",
      "\n",
      "**Graduate Courses**\n",
      "\n",
      "1. Master of Science in Information Technology (MSIT)\n",
      "2. Master of Science in Information Systems\n"
     ]
    }
   ],
   "source": [
    "# QUESTION: Use the Huggingface transformers library to load the Llama 3.2-3B instruct model\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "# Run the model locally. You do not have to use cloud-based APIs.\n",
    "\n",
    "# Execute the below query against the model and let it it answer from it's internal memory\n",
    "\n",
    "query = \"What courses are available in SUTD?\"\n",
    "\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (40 points)---\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", model=model_id, max_new_tokens=256, device = device # setting max_new_tokens to 512 to be able to run on my GPU\n",
    ")\n",
    "\n",
    "# TODO: fix\n",
    "output = pipeline(query)\n",
    "print(\"Model Response:\")\n",
    "print(output[0][\"generated_text\"])\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36af8f0a-45d0-404e-a1e9-8e16af4a158a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "models is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://huggingface.co/models/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\transformers\\utils\\hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:961\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1068\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1596\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1593\u001b[39m ):\n\u001b[32m   1594\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1595\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1485\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1401\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1400\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1401\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:285\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:309\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    308\u001b[39m response = get_session().request(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    450\u001b[39m     message = (\n\u001b[32m    451\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-67f0f2e3-1a8097af63d7f8cf190c4664;072f8c5f-99ec-43ce-adf5-9f7187c2f25b)\n\nRepository Not Found for url: https://huggingface.co/models/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     17\u001b[39m     MODEL_DIR,\n\u001b[32m     18\u001b[39m     torch_dtype=torch.float16,  \u001b[38;5;66;03m# using float16 for faster inference\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     low_cpu_mem_usage=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m pipeline = pipeline(\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer, max_new_tokens=\u001b[32m256\u001b[39m \u001b[38;5;66;03m# setting max_new_tokens to 512 to be able to run on my GPU\u001b[39;00m\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:910\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m    909\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m910\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m    912\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:742\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    739\u001b[39m     token = use_auth_token\n\u001b[32m    741\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    759\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\transformers\\utils\\hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chian\\Documents\\Code\\sutd_5055mlop-adi\\myenv\\Lib\\site-packages\\transformers\\utils\\hub.py:456\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    457\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    459\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    464\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: models is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "# QUESTION: Now put everything together. Use langchain to integrate your vector store and Llama model into a RAG system\n",
    "# Run the below example question against your RAG system.\n",
    "\n",
    "# Example questions\n",
    "# TODO: what does this mean?\n",
    "query = \"How can I increase my chances of admission into SUTD?\"\n",
    "\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (40 points)---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype=torch.float16,  # using float16 for faster inference\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,          # adding a 4-bit quantization\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256 # setting max_new_tokens to 512 to be able to run on my GPU\n",
    ")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "result = rag_chain.run(query)\n",
    "print(\"RAG Chain Response:\")\n",
    "print(result)\n",
    "#------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30ce26-8a19-4492-91e3-1cea5f750d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Below is set of test questions. Add another 10 test questions based on your user interviews and your value proposition canvas.\n",
    "# Run the complete set of test questions against the RAG question answering system.\n",
    "\n",
    "questions = [\"What are the admissions deadlines for SUTD?\",\n",
    "             \"Is there financial aid available?\",\n",
    "             \"What is the minimum score for the Mother Tongue Language?\",\n",
    "             \"Do I require reference letters?\",\n",
    "             \"Can polytechnic diploma students apply?\",\n",
    "             \"Do I need SAT score?\",\n",
    "             \"How many PhD students does SUTD have?\",\n",
    "             \"How much are the tuition fees for Singaporeans?\",\n",
    "             \"How much are the tuition fees for international students?\",\n",
    "             \"Is there a minimum CAP?\"\n",
    "             ]\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (20 points)---\n",
    "additional_questions = [\n",
    "    \"What is SUTD’s mission and vision?\",\n",
    "    \"When was SUTD officially inaugurated?\",\n",
    "    \"Which core values does SUTD emphasize?\",\n",
    "    \"Where is SUTD located, and how can it be contacted?\",\n",
    "    \"What different SUTD offices or departments can I reach out to?\",\n",
    "    \"What are the key components of the Freshmore curriculum at SUTD?\",\n",
    "    \"Which elective modules are available for Freshmore students in Term 3?\",\n",
    "    \"What courses are offered within the Design and Artificial Intelligence pillar?\",\n",
    "    \"Who are some of the instructors teaching the courses in the DAI program?\",\n",
    "    \"What are the main steps involved in the SUTD application process?\"\n",
    "]\n",
    "\n",
    "all_questions = questions + additional_questions\n",
    "for q in all_questions:\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Question: \" + q)\n",
    "    #run the RAG chain\n",
    "    result = rag_chain.run(q)\n",
    "    print(\"Response:\")\n",
    "    print(result)\n",
    "    print(\"----------------------------------------------------------------\\n\")#---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0fd068",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "\n",
    "\n",
    "Manually inspect each answer, fact check whether the answer is correct (use Google or any other method) and check the retrieved documents\n",
    "\n",
    "For each question, answer and context triple, record the following\n",
    "\n",
    "- How accurate is the answer (1-5, 5 best)?\n",
    "- How relevant is the retrieved context (1-5, 5 best)?\n",
    "- How grounded is the answer in the retrieved context (instead of relying on the LLM's internal knowledge) (1-5, 5 best)?\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (20 points) ---**\n",
    "\n",
    "\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67543217",
   "metadata": {},
   "source": [
    "You can try improve the chatbot by going back to previous steps in the notebook and change things until the submission deadline. For example, you can add more data sources, change the embedding models, change the data pre-processing, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3789e17-3fea-495d-a843-85f4b752812b",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "This concludes assignment 3.\n",
    "\n",
    "Please submit this notebook with your answers and the generated output cells as a **Jupyter notebook file** via github.\n",
    "\n",
    "\n",
    "Every group member should do the following submission steps:\n",
    "1. Create a private github repository **sutd_5055mlop** under your github user.\n",
    "2. Add your instructors as collaborator: ddahlmeier and lucainiaoge\n",
    "3. Save your submission as assignment_03_GROUP_NAME.ipynb where GROUP_NAME is the name of the group you have registered. \n",
    "4. Push the submission files to your repo \n",
    "5. Submit the link to the repo via eDimensions\n",
    "\n",
    "\n",
    "\n",
    "**Assignment due 6 April 2025 11:59pm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ad52c-8a4a-49be-bb75-17eaa4830706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
