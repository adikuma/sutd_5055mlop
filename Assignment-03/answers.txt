### QUESTION: What embeddings and vector store did you use and why? Explain your design decision in less than 10 sentences.
We used OpenAI's "text-embedding-ada-002" for embeddings and FAISS (Facebook AI Similarity Search) as our vector store. The "text-embedding-ada-002" model provides high-quality 1536-dimensional embeddings with excellent semantic understanding of academic and technical content, which is critical for accurately representing SUTD's educational information. FAISS was chosen for its efficient nearest-neighbor search capabilities optimized for high-dimensional vectors, fast retrieval performance with large document collections, and low memory footprint using quantization techniques. We implemented a custom embeddings class that supports both OpenAI and local HuggingFace models, allowing us to easily switch between services while maintaining the same interface. This combination delivers fast, accurate semantic search results while keeping the vector store locally persisted for reliability.

### QUESTION: What chunking method or strategy did you use? Why did you use this method. Explain your design decision in less than 10 sentences.
We implemented a hierarchical chunking strategy that preserves document structure by first extracting logical sections using markdown headers (h1-h6). Each section becomes a standalone chunk with its heading, content, and rich contextual metadata (title, URL, parent sections, pillar/department). Small chunks (<100 chars) are combined with nearby related chunks to avoid fragmentation, while large chunks (>1000 chars) are split using recursive character splitting with appropriate separators (paragraphs, sentences, etc.). We maintain a 100-character overlap between chunks to preserve context across boundaries. This approach preserves the semantic integrity of documents while keeping chunks optimally sized for embedding and retrieval, ensuring that retrieved chunks contain complete logical thought units rather than arbitrary text fragments.